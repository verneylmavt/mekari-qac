Great. Now that you understand the context of the entire 'Challenge Test', I want to explain a little bit about the solution. At high level, the solution is composed of 4 components: 'Data Processing and PostgreSQL Relational Database for Credit Card Transaction Dataset', 'Data Processing and Qdrant Vector Database for Credit Card Fraud Document', 'FastAPI Backend Server', and 'Streamlit Frontend UI'. In this chat, I want you to focus on: 'FastAPI Backend Server'.

Therefore, first I would like you to see the solution structure in 'Structure'. Then, thoroughly read and understand my entire solution for 'FastAPI Backend Server' in 'Code'.




'Structure':
```
mekari-qac
│
├── data/                                     # Dataset and data processing
│   ├── fraudData/
│   │   ├── fraudTrain.csv                    # Training split of credit card transaction dataset
│   │   ├── fraudTest.csv                     # Test split of credit card transaction dataset
│   │   ├── data_processing_fraudData.ipynb   # Data processing notebook for credit card transaction dataset
│   │   ├── fraudData_snapshot.dump           # DB snapshot
│   │   └── requirements.txt
│   │
│   └── Understanding Credit Card Frauds/
│       ├── Bhatla_Description.docx                                 # Credit card fraud document
│       ├── data_processing_Understanding Credit Card Frauds.ipynb  # Data processing notebook for credit card fraud document
│       ├── Bhatla_chunks.json                                      # Cleaned and segmented text chunks
│       ├── Bhatla_embeddings.npy                                   # Precomputed dense embeddings
│       └── requirements.txt
│
├── backend/                                  # FastAPI backend
│   ├── app/
│   │   ├── main.py                           # REST API: /health, /chat
│   │   ├── config.py                         # Environment variables + global configuration
│   │   ├── db.py                             # PostgreSQL engine creation + connection handling
│   │   ├── schemas.py                        # Pydantic request/response models
│   │   │
│   │   ├── agent/
│   │   │   ├── state.py                      # Central AgentState + shared memory fields
│   │   │   ├── graph.py                      # Routing graph: data, document, fallback, scoring
│   │   │   ├── router.py                     # LLM question router: data vs document vs none
│   │   │   ├── data_nodes.py                 # SQL generator, SQL executor, and data explanation nodes
│   │   │   ├── doc_nodes.py                  # Qdrant retrieval + RAG answer generator
│   │   │   └── scoring_node.py               # Quality-scoring node for evaluating LLM answers
│   │   │
│   │   ├── llm/
│   │   │   └── client.py                     # GPT-5-Nano/Mini wrappers for chat/completions
│   │   │
│   │   ├── rag/
│   │   │   └── qdrant_client.py              # Embedding, retrieval, reranking + Qdrant connection
│   │   │
│   │   └── repositories/
│   │       └── metrics_repo.py               # SQL execution helper for querying analytics tables/views
│   │
│   └── requirements.txt
│
├── frontend/                                 # Streamlit frontend
│   ├── app.py                                # Streamlit interface: health check, chat UI
│   └── requirements.txt
│
├── scripts/                                  # Initialization scripts
│   ├── init_postgresql.py                    # Script to initialize PostgreSQL
│   └── init_qdrant.py                        # Script to initialize Qdrant
│
├── assets/
│   ├── q&a_chatbot_fastapi_demo.mp4          # Demo video for FastAPI Backend Server
│   └── q&a_chatbot_streamlit_demo.mp4        # Demo video for Streamlit Frontend UI
│
├── .env
└── requirements.txt
```




'Code':

'backend/app/main.py'
```
from typing import Any, Dict, List

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy import text

from .config import get_settings
from .schemas import ChatRequest, ChatResponse
from .agent.graph import run_agent
from .db import get_engine
from .rag.qdrant_client import _qdrant_client  # type: ignore

settings = get_settings()

app = FastAPI(title="Fraud Q&A Chatbot")

# CORS (allow localhost frontends; tighten for prod)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
def health() -> Dict[str, Any]:
    # Simple health checks
    db_ok = False
    qdrant_ok = False

    try:
        engine = get_engine()
        with engine.connect() as conn:
            # SQLAlchemy 2.x: pass a text() object
            conn.execute(text("SELECT 1"))
        db_ok = True
    except Exception:
        db_ok = False

    try:
        _ = _qdrant_client.get_collections()
        qdrant_ok = True
    except Exception:
        qdrant_ok = False

    return {
        "status": "ok" if db_ok and qdrant_ok else "degraded",
        "db_ok": db_ok,
        "qdrant_ok": qdrant_ok,
        "model": settings.openai_model_name,
    }


@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest) -> ChatResponse:
    history_serialised: List[Dict[str, str]] = []
    if request.history:
        for m in request.history:
            history_serialised.append(
                {
                    "role": m.role,
                    "content": m.content,
                }
            )

    state = run_agent(
        question=request.question,
        history=history_serialised,
    )

    answer = state.get("answer") or ""
    answer_type = state.get("answer_type") or "unknown"
    quality = float(state.get("quality_score") or 0.0)
    sql = state.get("generated_sql")

    sources: List[Dict[str, Any]] = []

    if answer_type == "data":
        rows = state.get("sql_result_rows") or []
        sources.append(
            {
                "type": "sql_result",
                "rows_preview": rows[:5],
            }
        )
    elif answer_type == "document":
        chunks = state.get("context_chunks") or []
        doc_sources: List[Dict[str, Any]] = []
        for c in chunks[:5]:
            payload = c["payload"]
            doc_sources.append(
                {
                    "section": payload.get("section"),
                    "subsection": payload.get("subsection"),
                    "snippet": payload["text"][:300],
                    "rerank_score": c.get("rerank_score"),
                }
            )
        sources.append(
            {
                "type": "document_chunks",
                "chunks": doc_sources,
            }
        )

    return ChatResponse(
        answer=answer,
        answer_type=answer_type,
        quality_score=quality,
        sql=sql,
        sources=sources or None,
    )
```

'backend/app/config.py'
```
from functools import lru_cache

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    # Pydantic v2 style settings config
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )

    # DB
    db_host: str = Field("localhost", env="DB_HOST")
    db_port: int = Field(5432, env="DB_PORT")
    db_user: str = Field("user", env="DB_USER")
    db_password: str = Field("password", env="DB_PASSWORD")
    db_name: str = Field("database", env="DB_NAME")

    # Qdrant
    qdrant_url: str = Field("http://localhost:6333", env="QDRANT_URL")
    qdrant_collection: str = Field("bhatla_credit_fraud", env="QDRANT_COLLECTION")

    # OpenAI / GPT-5 Nano
    openai_api_key: str = Field(..., env="OPENAI_API_KEY")
    openai_model_name: str = Field("gpt-5-nano", env="OPENAI_MODEL_NAME")

    # Embedding / reranker
    embed_model_name: str = Field("BAAI/bge-base-en-v1.5", env="EMBED_MODEL_NAME")
    reranker_model_name: str = Field("BAAI/bge-reranker-base", env="RERANKER_MODEL_NAME")

    @property
    def database_url(self) -> str:
        return (
            f"postgresql+psycopg2://{self.db_user}:"
            f"{self.db_password}@{self.db_host}:{self.db_port}/{self.db_name}"
        )


@lru_cache
def get_settings() -> Settings:
    return Settings()
```

'backend/app/db.py'
```
from functools import lru_cache

from sqlalchemy import create_engine
from sqlalchemy.engine import Engine

from .config import get_settings


@lru_cache
def get_engine() -> Engine:
    settings = get_settings()
    engine = create_engine(
        settings.database_url,
        pool_pre_ping=True,
        future=True,
    )
    return engine
```

'backend/app/schemas.py'
```
from typing import List, Dict, Any, Optional

from pydantic import BaseModel, Field


class ChatMessage(BaseModel):
    role: str
    content: str


class ChatRequest(BaseModel):
    question: str
    history: Optional[List[ChatMessage]] = Field(default=None)


class ChatResponse(BaseModel):
    answer: str
    answer_type: str
    quality_score: float
    sql: Optional[str] = None
    sources: Optional[List[Dict[str, Any]]] = None
```

'backend/app/agent/state.py'
```
from typing import List, Dict, Any, Optional, Literal

from typing_extensions import TypedDict


class AgentState(TypedDict, total=False):
    question: str
    history: List[Dict[str, str]]
    route: Optional[Literal["data", "document", "none"]]

    generated_sql: Optional[str]
    sql_result_rows: Optional[List[Dict[str, Any]]]
    sql_result_preview: Optional[str]

    context_chunks: Optional[List[Dict[str, Any]]]

    answer: Optional[str]
    answer_type: Optional[str]
    quality_score: Optional[float]
```

'backend/app/agent/graph.py'
```
from typing import Dict, Any, List, Optional

from langgraph.graph import StateGraph, END

from .state import AgentState
from .router import router_node, fallback_answer_node
from .data_nodes import generate_sql_node, run_sql_node, data_answer_node
from .doc_nodes import retrieval_node, rag_answer_node
from .scoring_node import scoring_node


def _route_decider(state: AgentState) -> str:
    route = state.get("route")
    if route in ("data", "document", "none"):
        return route  # type: ignore
    # Default to none (fallback) if something weird happens
    return "none"


# Build LangGraph
_graph = StateGraph(AgentState)

_graph.add_node("router", router_node)
_graph.add_node("generate_sql", generate_sql_node)
_graph.add_node("run_sql", run_sql_node)
_graph.add_node("data_answer", data_answer_node)
_graph.add_node("retrieve", retrieval_node)
_graph.add_node("rag_answer", rag_answer_node)
_graph.add_node("fallback_answer", fallback_answer_node)
_graph.add_node("score", scoring_node)

_graph.set_entry_point("router")

_graph.add_conditional_edges(
    "router",
    _route_decider,
    {
        "data": "generate_sql",
        "document": "retrieve",
        "none": "fallback_answer",
    },
)

_graph.add_edge("generate_sql", "run_sql")
_graph.add_edge("run_sql", "data_answer")
_graph.add_edge("data_answer", "score")

_graph.add_edge("retrieve", "rag_answer")
_graph.add_edge("rag_answer", "score")

_graph.add_edge("fallback_answer", "score")

_graph.add_edge("score", END)

_app = _graph.compile()


def run_agent(
    question: str,
    history: Optional[List[Dict[str, str]]] = None,
) -> AgentState:
    initial_state: AgentState = {
        "question": question,
        "history": history or [],
    }
    final_state: AgentState = _app.invoke(initial_state)
    return final_state
```

'backend/app/agent/router.py'
```
from .state import AgentState
from ..llm.client import call_gpt5_nano, call_gpt5_mini

ROUTER_SYSTEM_PROMPT = (
    "You are a router for an internal fraud-analytics assistant.\n"
    "This assistant has access to exactly two internal knowledge sources:\n\n"
    "1) data: tabular credit-card transaction data\n"
    "   This data contains credit-card transactions where each row represents a single purchase "
    "   made by a cardholder and includes detailed information about the transaction time, amount, "
    "   merchant, category, customer demographics, home location, merchant location, and a binary "
    "   label indicating whether the transaction is fraudulent.\n\n"
    "2) document: a conceptual white paper called \"Understanding Credit Card Frauds\"\n"
    "   This document contains a 2003-era white paper that explains the growing problem of credit "
    "   card fraud, detailing how fraud is committed (including application fraud, lost/stolen and "
    "   counterfeit cards, skimming, merchant collusion, triangulation schemes, and internet-based "
    "   attacks such as site cloning, fake merchant sites, and card number generators), "
    "   quantifying global and country-specific loss trends, and analyzing the impact on cardholders "
    "   (limited liability), merchants (full liability, chargebacks, fees, admin overhead, and "
    "   reputation damage), and banks (direct losses plus high prevention and operational costs). "
    "   It reviews both basic and advanced fraud prevention methods—manual review, Address "
    "   Verification System, card verification codes, negative/positive lists, payer authentication "
    "   (e.g., Verified by Visa), lockout mechanisms, and blacklists of fraudulent merchants—then "
    "   describes more sophisticated techniques such as rule-based systems, statistical risk "
    "   scoring, neural networks, biometrics, and smart card (EMV) technology. The paper’s central "
    "   thesis is that effective fraud management is about minimizing the \"total cost of fraud\"—the "
    "   sum of actual fraud losses and the cost of prevention—by using these tools to segment and "
    "   prioritize transactions so that only the riskiest subset is subject to intensive review, "
    "   thereby achieving an optimal balance between security, cost, and customer experience.\n\n"
    "Your task:\n"
    "- Choose 'data' if the question is primarily about analyzing the tabular credit-card "
    "  transaction data (e.g., fraud rates, time trends, top merchants/categories, transaction "
    "  patterns, customer or merchant-level statistics).\n"
    "- Choose 'document' if the question is primarily about fraud concepts, mechanisms, definitions, "
    "  or the authors' opinions as described in the white paper.\n"
    "- Choose 'none' if the question is clearly unrelated to both the transaction dataset and the "
    "  document, or if it cannot reasonably be answered using these two sources.\n\n"
    "Return exactly one word: data, document, or none."
)


def router_node(state: AgentState) -> AgentState:
    question = state["question"]
    user_prompt = f"Question: {question}\n\nAnswer with exactly one word: data, document, or none."

    raw = call_gpt5_nano(
        ROUTER_SYSTEM_PROMPT,
        user_prompt,
        temperature=0.0,
        max_tokens=4,
    )
    print("Router (by GPT-5 Nano) is Called")
    route = raw.strip().lower()

    if route not in ("data", "document", "none"):
        # Fallback heuristic if the model returns something unexpected
        q_lower = question.lower()

        # Heuristic for data questions
        if any(
            word in q_lower
            for word in [
                "rate",
                "trend",
                "daily",
                "monthly",
                "time series",
                "merchant",
                "category",
                "transaction",
                "amount",
                "volume",
                "count",
                "share",
                "proportion",
                "distribution",
            ]
        ):
            route = "data"
        # Heuristic for document questions
        elif any(
            word in q_lower
            for word in [
                "application fraud",
                "lost/stolen",
                "lost or stolen",
                "counterfeit",
                "skimming",
                "merchant collusion",
                "triangulation",
                "internet",
                "site cloning",
                "neural network",
                "fraud detection system",
                "total cost of fraud",
                "prevention",
                "white paper",
            ]
        ):
            route = "document"
        else:
            route = "none"

    state["route"] = route  # type: ignore
    return state


FALLBACK_SYSTEM_PROMPT = (
    "You are an assistant for an internal fraud-analytics tool. This tool is limited to:\n"
    "- A specific credit-card transaction dataset (tabular data with transactions and fraud labels).\n"
    "- A specific conceptual document: \"Understanding Credit Card Frauds\" (a 2003-era white paper).\n\n"
    "If the user's question is outside the scope of these two resources, you must NOT pretend to "
    "answer it using unrelated knowledge. Instead, explain clearly that this particular chatbot "
    "is specialized for that dataset and document only, and that the question appears to be "
    "outside its domain.\n\n"
    "Be brief and honest. Do NOT mention any internal routing logic."
)


def fallback_answer_node(state: AgentState) -> AgentState:
    question = state["question"]

    user_prompt = (
        f"User question:\n{question}\n\n"
        "Explain that this tool is limited to the fraud dataset and the fraud document described "
        "in the system prompt, and that the question does not seem to be answerable within that scope."
    )

    answer = call_gpt5_mini(
        system_prompt=FALLBACK_SYSTEM_PROMPT,
        user_prompt=user_prompt,
        temperature=0.2,
        max_tokens=256,
    )

    state["answer"] = answer  # type: ignore
    state["answer_type"] = "other"  # type: ignore
    return state
```

'backend/app/agent/data_nodes.py'
```
from typing import List
from decimal import Decimal
from statistics import mean
import numbers

from .state import AgentState
from ..llm.client import call_gpt5_mini
from ..repositories.metrics_repo import run_sql_query


SCHEMA_DESCRIPTION = """
You can query the following PostgreSQL tables and materialized views.

Dimension tables:

1) dim_customer(
    customer_id BIGINT,
    cc_num TEXT,
    first TEXT,
    last TEXT,
    gender VARCHAR(1),
    street TEXT,
    city TEXT,
    state TEXT,
    zip TEXT,
    lat DOUBLE PRECISION,
    long DOUBLE PRECISION,
    city_pop BIGINT,
    job TEXT,
    dob DATE
)

2) dim_merchant(
    merchant_id BIGINT,
    merchant_name TEXT,
    merch_lat DOUBLE PRECISION,
    merch_long DOUBLE PRECISION
)

3) dim_category(
    category_id BIGINT,
    category_name TEXT
)

4) dim_date(
    date_id BIGINT,
    trans_date DATE,
    year INT,
    month INT,
    day INT,
    day_of_week TEXT,
    is_weekend BOOLEAN,
    year_month TEXT
)

Fact table:

5) fact_transactions(
    transaction_id BIGINT,
    trans_num TEXT,
    customer_id BIGINT,
    merchant_id BIGINT,
    category_id BIGINT,
    date_id BIGINT,
    trans_ts TIMESTAMP,
    unix_time BIGINT,
    amt DOUBLE PRECISION,
    is_fraud SMALLINT,
    year INT,
    month INT,
    hour INT,
    is_weekend BOOLEAN,
    cust_merch_distance_km DOUBLE PRECISION,
    split TEXT
)

Pre-aggregated materialized views (preferred when appropriate):

6) agg_daily_fraud(
    trans_date DATE,
    year INT,
    month INT,
    day INT,
    day_of_week TEXT,
    is_weekend BOOLEAN,
    year_month TEXT,
    total_tx BIGINT,
    fraud_tx BIGINT,
    fraud_rate DOUBLE PRECISION,
    total_amount DOUBLE PRECISION,
    fraud_amount DOUBLE PRECISION,
    fraud_share_by_value DOUBLE PRECISION
)

7) agg_monthly_fraud(
    year INT,
    month INT,
    year_month TEXT,
    total_tx BIGINT,
    fraud_tx BIGINT,
    fraud_rate DOUBLE PRECISION,
    total_amount DOUBLE PRECISION,
    fraud_amount DOUBLE PRECISION,
    fraud_share_by_value DOUBLE PRECISION
)

8) agg_merchant_fraud(
    merchant_id BIGINT,
    merchant_name TEXT,
    total_tx BIGINT,
    fraud_tx BIGINT,
    fraud_rate DOUBLE PRECISION,
    total_amount DOUBLE PRECISION,
    fraud_amount DOUBLE PRECISION,
    fraud_share_by_value DOUBLE PRECISION
)

9) agg_category_fraud(
    category_id BIGINT,
    category_name TEXT,
    total_tx BIGINT,
    fraud_tx BIGINT,
    fraud_rate DOUBLE PRECISION,
    total_amount DOUBLE PRECISION,
    fraud_amount DOUBLE PRECISION,
    fraud_share_by_value DOUBLE PRECISION
)

Guidance:

- Whenever possible, prefer the agg_* materialized views for questions about overall
  daily/monthly fraud rates, top merchants, top categories, and similar aggregated metrics.
- If the question requires raw transaction-level details (e.g., specific customers, card numbers,
  time-of-day patterns, individual transactions) or metrics that are not present in the views,
  then use fact_transactions and join it to the dimension tables as needed:
  - fact_transactions.customer_id = dim_customer.customer_id
  - fact_transactions.merchant_id = dim_merchant.merchant_id
  - fact_transactions.category_id = dim_category.category_id
  - fact_transactions.date_id = dim_date.date_id

Examples:

Q: "How does the monthly fraud rate evolve over the entire period?"
SQL:
  SELECT year_month, fraud_rate
  FROM agg_monthly_fraud
  ORDER BY year, month;

Q: "Which merchants have the highest fraud rate?"
SQL:
  SELECT merchant_name, total_tx, fraud_tx, fraud_rate
  FROM agg_merchant_fraud
  ORDER BY fraud_rate DESC
  LIMIT 10;

Q: "Which merchant categories exhibit the highest incidence of fraudulent transactions?"
SQL:
  SELECT category_name, total_tx, fraud_tx, fraud_rate
  FROM agg_category_fraud
  ORDER BY fraud_rate DESC
  LIMIT 10;

Q: "List the last 10 fraudulent transactions with customer name and merchant."
SQL:
  SELECT
      f.trans_ts,
      c.first AS customer_first,
      c.last AS customer_last,
      m.merchant_name,
      f.amt,
      f.is_fraud
  FROM fact_transactions f
  JOIN dim_customer c ON f.customer_id = c.customer_id
  JOIN dim_merchant m ON f.merchant_id = m.merchant_id
  WHERE f.is_fraud = 1
  ORDER BY f.trans_ts DESC
  LIMIT 10;

Q: "What is the average transaction amount by hour of day for fraudulent transactions?"
SQL:
  SELECT
      f.hour,
      AVG(f.amt) AS avg_fraud_amount,
      COUNT(*) AS fraud_tx
  FROM fact_transactions f
  WHERE f.is_fraud = 1
  GROUP BY f.hour
  ORDER BY f.hour;
"""

SQL_GEN_SYSTEM = (
    "You generate a single PostgreSQL SELECT query against the tables and views described below.\n"
    "Rules:\n"
    "- You may use any of the listed tables and views.\n"
    "- Prefer the agg_* materialized views when they already contain the metrics needed.\n"
    "- If the views are not sufficient to answer the question (for example, if you need "
    "transaction-level detail, specific customers, card numbers, or time-of-day patterns), "
    "then use fact_transactions and join it with the dimension tables as appropriate.\n"
    "- Only reference the tables and views that are listed in the schema description.\n"
    "- Do not add comments.\n"
    "- Do not wrap the query in backticks.\n\n"
    f"{SCHEMA_DESCRIPTION}"
)


def generate_sql_node(state: AgentState) -> AgentState:
    question = state["question"]
    user_prompt = (
        "Write a single PostgreSQL SELECT statement that answers the question.\n"
        f"Question: {question}\n\n"
        "Output only the SQL query."
    )
    sql = call_gpt5_mini(
        system_prompt=SQL_GEN_SYSTEM,
        user_prompt=user_prompt,
        temperature=0.0,
        max_tokens=512,
    )
    print("Text-to-SQL (by GPT-5 Mini) is Called")
    sql = sql.strip().strip(";")
    state["generated_sql"] = sql  # type: ignore
    return state


def _is_numeric(value) -> bool:
    if isinstance(value, (int, float)):
        return True
    if isinstance(value, Decimal):
        return True
    if isinstance(value, numbers.Real) and not isinstance(value, bool):
        return True
    return False


def run_sql_node(state: AgentState) -> AgentState:
    sql = state.get("generated_sql")
    if not sql:
        state["sql_result_rows"] = []  # type: ignore
        state["sql_result_preview"] = "No SQL generated."
        return state

    try:
        print("Querying Data from Relational Database...")
        rows = run_sql_query(sql, default_limit=200)
        state["sql_result_rows"] = rows  # type: ignore

        preview_lines: List[str] = []
        preview_lines.append(
            f"Total rows returned (capped at backend limit): {len(rows)}"
        )

        # Show up to 5 example rows
        max_preview_rows = 5
        for idx, row in enumerate(rows[:max_preview_rows]):
            preview_lines.append(f"Row {idx+1}: {row}")

        # Generic numeric summary statistics across all returned rows
        if rows:
            first_row = rows[0]
            numeric_fields = [
                k for k, v in first_row.items() if _is_numeric(v)
            ]

            stats_lines: List[str] = []
            for field in numeric_fields:
                values: List[float] = []
                for r in rows:
                    v = r.get(field)
                    if _is_numeric(v):
                        if isinstance(v, Decimal):
                            v = float(v)
                        values.append(float(v))
                if values:
                    stats_lines.append(
                        f"Summary for '{field}': "
                        f"min={min(values):.6g}, "
                        f"mean={mean(values):.6g}, "
                        f"max={max(values):.6g}"
                    )

            if stats_lines:
                preview_lines.append("Summary statistics over all returned rows:")
                preview_lines.extend(stats_lines)

        state["sql_result_preview"] = (
            "\n".join(preview_lines) if preview_lines else "No rows returned."
        )
    except Exception as e:
        state["sql_result_rows"] = []  # type: ignore
        state["sql_result_preview"] = f"Error executing SQL: {e}"

    return state


DATA_ANSWER_SYSTEM = (
    "You are a data analyst. You are given:\n"
    "- A user question.\n"
    "- The SQL query used to answer it.\n"
    "- A preview of the result rows, which may include summary statistics.\n\n"
    "Your job:\n"
    "1) First, directly answer the user's question using the information from the SQL results "
    "(including any summary statistics). Use 1–3 short paragraphs.\n"
    "2) Then, briefly mention any important limitations or caveats (for example, if the result "
    "set is small, heavily filtered, or if the query may not perfectly match the question).\n"
    "3) Do not spend more than one short paragraph describing the SQL itself.\n"
    "If the result is empty or if there was an error, clearly say that and explain what might be wrong."
)


def data_answer_node(state: AgentState) -> AgentState:
    question = state["question"]
    sql = state.get("generated_sql") or ""
    preview = state.get("sql_result_preview") or ""

    user_prompt = (
        f"Question:\n{question}\n\n"
        f"SQL used:\n{sql}\n\n"
        f"Result preview:\n{preview}\n\n"
        "Now provide a concise but informative answer to the user, following the instructions above."
    )

    answer = call_gpt5_mini(
        system_prompt=DATA_ANSWER_SYSTEM,
        user_prompt=user_prompt,
        temperature=0.2,
        max_tokens=512,
    )
    print("Reasoning for Queried Data (by GPT-5 Mini) is Called")
    
    state["answer"] = answer  # type: ignore
    state["answer_type"] = "data"  # type: ignore
    return state
```

'backend/app/agent/doc_nodes.py'
```
from typing import List, Dict, Any

from .state import AgentState
from ..rag.qdrant_client import retrieve_relevant_chunks
from ..llm.client import call_gpt5_mini


def retrieval_node(state: AgentState) -> AgentState:
    question = state["question"]
    print("Retrieving Document from Vector Database...")
    results = retrieve_relevant_chunks(question, top_k=10, use_reranker=True)
    state["context_chunks"] = results  # type: ignore
    return state


DOC_ANSWER_SYSTEM = (
    "You are answering questions strictly based on the document "
    "“Understanding Credit Card Frauds”.\n\n"
    "You will be given several relevant excerpts from the document. "
    "Using only these excerpts (do not invent facts), answer the question.\n"
    "If the excerpts do not contain enough information to fully answer, "
    "say so clearly."
)


def _build_context(
    chunks: List[Dict[str, Any]],
    max_chars_per_chunk: int = 500,
) -> str:
    parts: List[str] = []
    for i, c in enumerate(chunks, start=1):
        payload = c["payload"]
        section = payload.get("section")
        subsection = payload.get("subsection")
        text = payload["text"].replace("\n", " ")
        snippet = text[:max_chars_per_chunk]
        parts.append(
            f"[{i}] Section: {section} | Subsection: {subsection}\n{snippet}\n"
        )
    return "\n".join(parts)


def rag_answer_node(state: AgentState) -> AgentState:
    question = state["question"]
    chunks = state.get("context_chunks") or []

    context = _build_context(chunks)

    user_prompt = (
        f"Excerpts from the document:\n{context}\n\n"
        f"Question: {question}\n\n"
        "Answer in clear language and refer implicitly to the document's content."
    )

    answer = call_gpt5_mini(
        system_prompt=DOC_ANSWER_SYSTEM,
        user_prompt=user_prompt,
        temperature=0.2,
        max_tokens=512,
    )
    print("Reasoning for Retrieved Document (by GPT-5 Mini) is Called")
    
    state["answer"] = answer  # type: ignore
    state["answer_type"] = "document"  # type: ignore
    return state
```

'backend/app/agent/scoring_node.py'
```
from typing import List, Dict, Any

from .state import AgentState
from ..llm.client import call_gpt5_nano

SCORE_SYSTEM = (
    "You are grading an answer.\n"
    "Given a question, an answer, and some evidence, you must output a single number "
    "between 0.0 and 1.0 (inclusive) representing how correct and well-supported the answer is.\n"
    "0.0 means completely incorrect or unsupported. 1.0 means fully correct and well supported.\n"
    "Return only the number, nothing else."
)


def scoring_node(state: AgentState) -> AgentState:
    question = state["question"]
    answer = state.get("answer") or ""

    # Heuristic base score
    base_score = 0.5

    if state.get("answer_type") == "data":
        rows = state.get("sql_result_rows") or []
        preview = state.get("sql_result_preview") or ""
        if rows and "Error executing SQL" not in preview:
            base_score = 0.75
        elif not rows:
            base_score = 0.3
        evidence = preview
    else:
        chunks: List[Dict[str, Any]] = state.get("context_chunks") or []
        if chunks:
            avg_rerank = sum(c.get("rerank_score", 0.0) for c in chunks) / len(chunks)
            # crude sigmoid mapping to [0,1]
            norm = 1 / (1 + pow(2.71828, -avg_rerank))
            base_score = 0.6 + 0.3 * norm
        else:
            base_score = 0.3

        snippets: List[str] = []
        for c in chunks[:2]:
            text = c["payload"]["text"].replace("\n", " ")
            snippets.append(text[:300])
        evidence = "\n\n".join(snippets)

    user_prompt = (
        f"Question:\n{question}\n\n"
        f"Answer:\n{answer}\n\n"
        f"Evidence:\n{evidence}\n\n"
        "Score:"
    )

    try:
        raw = call_gpt5_nano(
            system_prompt=SCORE_SYSTEM,
            user_prompt=user_prompt,
            temperature=0.0,
            max_tokens=10,
        )
        print("Scoring (by GPT-5 Nano) is Called")
        raw = raw.strip()
        score = float(raw)
        score = max(0.0, min(1.0, score))
    except Exception:
        score = base_score

    final_score = (base_score + score) / 2.0
    state["quality_score"] = final_score  # type: ignore
    return state
```

'backend/app/llm/client.py'
```
from typing import Optional

from openai import OpenAI

from ..config import get_settings

settings = get_settings()

_client = OpenAI(api_key=settings.openai_api_key)


def call_gpt5_mini(
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.0,
    max_tokens: int = 512,
) -> str:
    """
    Generic wrapper to call the GPT-5 Mini model (or any model name provided).

    Uses openai==2.9.0 style:
    client = OpenAI()
    client.chat.completions.create(...)
    """
    completion = _client.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        # temperature=temperature,
        # max_completion_tokens=max_tokens,
    )

    content: Optional[str] = completion.choices[0].message.content
    # print("GPT-5 Mini is Called")
    return (content or "").strip()


def call_gpt5_nano(
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.0,
    max_tokens: int = 512,
) -> str:
    """
    Generic wrapper to call the GPT-5 Nano model (or any model name provided).

    Uses openai==2.9.0 style:
    client = OpenAI()
    client.chat.completions.create(...)
    """
    completion = _client.chat.completions.create(
        model="gpt-5-nano",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        # temperature=temperature,
        # max_completion_tokens=max_tokens,
    )

    content: Optional[str] = completion.choices[0].message.content
    # print("GPT-5 Nano is Called")
    return (content or "").strip()
```

'backend/app/rag/qdrant_client.py'
```
from typing import List, Dict, Any, Optional

import numpy as np
import torch
from sentence_transformers import SentenceTransformer, CrossEncoder
from qdrant_client import QdrantClient
from qdrant_client.http.models import QueryResponse
from qdrant_client.http import models as qmodels

from ..config import get_settings

settings = get_settings()

# Device selection
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load embedding model (BGE)
_embed_model = SentenceTransformer(settings.embed_model_name, device=device)

# Load reranker model (BGE reranker)
_reranker = CrossEncoder(
    settings.reranker_model_name,
    device=device,
    max_length=512,
    trust_remote_code=True,
)

# Qdrant client
_qdrant_client = QdrantClient(url=settings.qdrant_url)
_collection_name = settings.qdrant_collection


def embed_texts(texts: List[str], batch_size: int = 32, is_query: bool = False) -> np.ndarray:
    if is_query:
        prefixed = [f"query: {t}" for t in texts]
    else:
        prefixed = [f"passage: {t}" for t in texts]

    embeddings = _embed_model.encode(
        prefixed,
        batch_size=batch_size,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False,
    )
    return embeddings


def embed_query(query: str) -> np.ndarray:
    return embed_texts([query], batch_size=1, is_query=True)[0]


def search_qdrant(
    query: str,
    top_k: int = 20,
) -> List[Dict[str, Any]]:
    query_vector = embed_query(query)

    response: QueryResponse = _qdrant_client.query_points(
        collection_name=_collection_name,
        query=query_vector.tolist(),
        limit=top_k,
        with_payload=True,
    )

    output: List[Dict[str, Any]] = []
    for p in response.points:
        output.append(
            {
                "id": p.id,
                "score": p.score,
                "payload": p.payload,
            }
        )
    return output


def rerank_with_bge_reranker(
    query: str,
    retrieved_results: List[Dict[str, Any]],
    top_k: Optional[int] = None,
) -> List[Dict[str, Any]]:
    if not retrieved_results:
        return []

    texts = [r["payload"]["text"] for r in retrieved_results]
    pairs = [(query, t) for t in texts]

    scores = _reranker.predict(pairs)

    for r, s in zip(retrieved_results, scores):
        r["rerank_score"] = float(s)

    reranked = sorted(retrieved_results, key=lambda x: x["rerank_score"], reverse=True)

    if top_k is not None:
        reranked = reranked[:top_k]

    return reranked


def retrieve_relevant_chunks(
    query: str,
    top_k: int = 5,
    use_reranker: bool = True,
) -> List[Dict[str, Any]]:
    dense_results = search_qdrant(query, top_k=top_k)
    if not use_reranker:
        return dense_results
    return rerank_with_bge_reranker(query, dense_results, top_k=top_k)
```

'backend/app/repositories/metrics_repo.py'
```
from typing import List, Dict, Any, Optional
import re  # NEW

from sqlalchemy import text
from sqlalchemy.engine import Engine

from ..db import get_engine


def _rows_to_dicts(result) -> List[Dict[str, Any]]:
    # SQLAlchemy 2.x friendly: use .mappings()
    return [dict(row) for row in result.mappings().all()]


def validate_sql(sql: str) -> str:
    """
    Very simple SQL safety: enforce SELECT and prevent semicolons / dangerous keywords.
    """
    cleaned = sql.strip().rstrip(";")
    lowered = cleaned.lower()

    # if not lowered.startswith("select"):
    #     raise ValueError("Only SELECT statements are allowed.")

    dangerous = [
        "insert ",
        "update ",
        "delete ",
        "drop ",
        "create ",
        "alter ",
        "truncate ",
    ]
    if any(word in lowered for word in dangerous):
        raise ValueError("SQL contains potentially dangerous keywords.")

    return cleaned


def run_sql_query(
    sql: str,
    engine: Optional[Engine] = None,
    default_limit: int = 200,
) -> List[Dict[str, Any]]:
    engine = engine or get_engine()
    safe_sql = validate_sql(sql)

    with engine.connect() as conn:
        result = conn.execute(text(safe_sql))
        return _rows_to_dicts(result)
```